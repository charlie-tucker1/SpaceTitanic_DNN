{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f0d6adb-abc5-449a-858f-b36e607c1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2e9ded84-b4c2-4b54-a9e6-beb93f347d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\n",
      "Imputing and Scaling...\n",
      "Encoding\n",
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train_data = pd.read_csv(r\"C:\\Users\\charl\\Space_titanic\\train.csv\")\n",
    "test_data = pd.read_csv(r\"C:\\Users\\charl\\Space_titanic\\test.csv\")\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "def process_data(df):\n",
    "    # Split Cabin info (B/0/P -> B, 0, P)\n",
    "    df[['CabinDeck', 'CabinNum', 'CabinSide']] = df['Cabin'].str.split('/', expand=True)\n",
    "    \n",
    "    # Convert CabinNum to float\n",
    "    df['CabinNum'] = pd.to_numeric(df['CabinNum'], errors='coerce')\n",
    "    \n",
    "    # Drop columns that cause overfitting/are unused\n",
    "    df.drop(columns=['Name', 'Cabin', 'PassengerId'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Log Transform Spending\n",
    "    spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "    df['TotalSpend'] = df[spend_cols].sum(axis=1) \n",
    "    \n",
    "    # Apply Log1p to handle skewness\n",
    "    for col in spend_cols + ['TotalSpend']:\n",
    "        df[col] = df[col].fillna(0)\n",
    "        df[col] = np.log1p(df[col])\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply Feature Engineering\n",
    "print(\"Processing data\")\n",
    "\n",
    "# Save ids\n",
    "test_passenger_ids = test_data['PassengerId'].copy()\n",
    "\n",
    "# Then process\n",
    "train_data = process_data(train_data.drop(columns=['Transported']))\n",
    "test_data = process_data(test_data)\n",
    "\n",
    "# Config\n",
    "cat_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'CabinDeck', 'CabinSide']\n",
    "num_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'CabinNum', 'TotalSpend']\n",
    "\n",
    "# Imputation and Scaling\n",
    "print(\"Imputing and Scaling...\")\n",
    "\n",
    "# Impute Numbers (Median)\n",
    "imputer_num = SimpleImputer(strategy='median')\n",
    "train_data[num_cols] = imputer_num.fit_transform(train_data[num_cols])\n",
    "test_data[num_cols] = imputer_num.transform(test_data[num_cols])\n",
    "\n",
    "# Scale Numbers (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "train_data[num_cols] = scaler.fit_transform(train_data[num_cols])\n",
    "test_data[num_cols] = scaler.transform(test_data[num_cols])\n",
    "\n",
    "# Impute Categories (Mode)\n",
    "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "train_data[cat_cols] = imputer_cat.fit_transform(train_data[cat_cols])\n",
    "test_data[cat_cols] = imputer_cat.transform(test_data[cat_cols])\n",
    "\n",
    "# Encoding\n",
    "print(\"Encoding\")\n",
    "embedding_sizes = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Fit on all data\n",
    "    all_values = pd.concat([train_data[col], test_data[col]], axis=0)\n",
    "    le.fit(all_values)\n",
    "    \n",
    "    train_data[col] = le.transform(train_data[col])\n",
    "    test_data[col] = le.transform(test_data[col])\n",
    "    \n",
    "    # Calculate Embedding dimensions\n",
    "    input_size = len(le.classes_)\n",
    "    output_dim = min(50, (input_size + 1) // 2)\n",
    "    embedding_sizes.append((input_size, output_dim))\n",
    "print(\"Preprocessing done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d4af8cb2-db2f-4a09-a9c3-a75d06d1eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make torch dataset class\n",
    "\n",
    "class SpaceTitanicDataset(Dataset):\n",
    "    def __init__(self, X, y=None, embedded_col_names=None):\n",
    "        \"\"\"\n",
    "        X: The DataFrame containing input features\n",
    "        y: The Series containing labels (Transported or not). None for test set.\n",
    "        embedded_col_names: List of column names that are categorical\n",
    "        \"\"\"\n",
    "        # 1. Setup the Categorical Data\n",
    "        if embedded_col_names:\n",
    "            self.X_cat = X[embedded_col_names].copy().values.astype('int64') # Ints for lookup\n",
    "            #Drop to avoid duplication\n",
    "            self.X_cont = X.drop(columns=embedded_col_names).copy().values.astype('float32')\n",
    "        else:\n",
    "            self.X_cat = None\n",
    "            self.X_cont = X.values.astype('float32')\n",
    "            \n",
    "        # 2. Setup the Label\n",
    "        if y is not None:\n",
    "            self.y = y.values.astype('float32').reshape(-1, 1) #Reshape into a column vector\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.X_cont)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x_cont = self.X_cont[idx]\n",
    "        \n",
    "        if self.X_cat is not None:\n",
    "            x_cat = self.X_cat[idx]\n",
    "        else:\n",
    "            x_cat = [] \n",
    "            \n",
    "        if self.y is not None:\n",
    "            return x_cat, x_cont, self.y[idx]\n",
    "        else:\n",
    "            return x_cat, x_cont "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6ce00870-27d7-4c33-bcd1-632432a11bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Model architecture\n",
    "class SpaceTitanicModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont_features, dropout_p=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_sizes (list of tuples): List of (num_categories, embedding_dim) \n",
    "            n_cont_features (int): Number of continuous features (Age, Spend, etc.)\n",
    "            dropout_p (float): Default is 30%\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(categories, size) for categories, size in embedding_sizes\n",
    "        ])\n",
    "        \n",
    "        # Length of embedding features\n",
    "        self.n_emb = sum(e.embedding_dim for e in self.embeddings)\n",
    "        self.n_cont = n_cont_features\n",
    "        \n",
    "        # Total Input Size = Embeddings + Continuous features\n",
    "        self.in_features = self.n_emb + self.n_cont\n",
    "\n",
    "        \n",
    "        # Layers of the model\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(self.in_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p)\n",
    "        )\n",
    "        \n",
    "        # Output Layer\n",
    "        self.out = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        # Embeddings\n",
    "        embeddings = []\n",
    "        for i, e in enumerate(self.embeddings):\n",
    "            # Lookup the vector for the i-th categorical column\n",
    "            embeddings.append(e(x_cat[:, i]))\n",
    "            \n",
    "        # Concatenate embeddings horizontally (Batch, Total_Emb_Dim)\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        \n",
    "        # Concat with continuous features(Batch, Total_Emb_Dim + n_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        \n",
    "        # Feed forward\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        # Final output\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58056911-d0df-4beb-ade7-2e252e6e3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Continuous Features size\n",
    "n_cont_features = len(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5c50b5cb-1279-4261-9da1-ec0faac6f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_ds = SpaceTitanicDataset(X_train, y_train, embedded_col_names=cat_cols)\n",
    "val_ds = SpaceTitanicDataset(X_val, y_val, embedded_col_names=cat_cols)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = SpaceTitanicModel(embedding_sizes, n_cont_features, dropout_p=0.4)\n",
    "model.to(device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d50c2908-4356-4b7e-8d18-3401fa86e20e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 | Train Loss: 0.4566 Acc: 78.43% | Val Loss: 0.4166 Acc: 79.59%\n",
      "Epoch  10 | Train Loss: 0.4423 Acc: 79.22% | Val Loss: 0.4099 Acc: 79.93%\n",
      "Epoch  15 | Train Loss: 0.4293 Acc: 79.38% | Val Loss: 0.4032 Acc: 80.68%\n",
      "Epoch  20 | Train Loss: 0.4238 Acc: 80.16% | Val Loss: 0.4002 Acc: 80.22%\n",
      "Epoch  25 | Train Loss: 0.4176 Acc: 80.43% | Val Loss: 0.3972 Acc: 80.22%\n",
      "Epoch  30 | Train Loss: 0.4104 Acc: 80.77% | Val Loss: 0.3930 Acc: 80.79%\n",
      "Epoch  35 | Train Loss: 0.4079 Acc: 80.53% | Val Loss: 0.3955 Acc: 79.64%\n",
      "Early stopping at epoch 38\n",
      "\n",
      "Best Validation Accuracy: 81.08%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 100\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "early_stop_patience = 15\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_cat, x_cont, y in train_dl:\n",
    "        x_cat, x_cont, y = x_cat.to(device), x_cont.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_cat, x_cont)\n",
    "        loss = loss_func(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (predicted == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    \n",
    "    train_loss = running_loss / len(train_dl)\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    #Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_cont, y in val_dl:\n",
    "            x_cat, x_cont, y = x_cat.to(device), x_cont.to(device), y.to(device)\n",
    "            outputs = model(x_cat, x_cont)\n",
    "            val_loss += loss_func(outputs, y).item()\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "            val_total += y.size(0)\n",
    "    \n",
    "    val_loss /= len(val_dl)\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')  # Save best\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train Loss: {train_loss:.4f} Acc: {train_acc*100:.2f}% | Val Loss: {val_loss:.4f} Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    if patience_counter >= early_stop_patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_val_acc*100:.2f}%\")\n",
    "model.load_state_dict(torch.load('best_model.pt'))  # Load best model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d05f291a-619d-4fe3-abf3-3c72cd1371c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Saved to: C:\\Users\\charl\\Titanic_notebook\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "print(\"Testing...\")\n",
    "\n",
    "# Create test dataset\n",
    "test_ds = SpaceTitanicDataset(test_data, y=None, embedded_col_names=cat_cols)\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_cat, x_cont in test_dl:\n",
    "        x_cat = x_cat.to(device)\n",
    "        x_cont = x_cont.to(device)\n",
    "        \n",
    "        outputs = model(x_cat, x_cont)\n",
    "        \n",
    "        preds = (torch.sigmoid(outputs) > 0.5).cpu().numpy().flatten()\n",
    "        all_predictions.extend(preds)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Transported': all_predictions\n",
    "})\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Save there\n",
    "submission_df.to_csv(os.path.join(notebook_dir, 'submission.csv'), index=False)\n",
    "\n",
    "print(f\"Saved to: {os.path.join(notebook_dir, 'submission.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc864988-8d03-4694-aaf7-3565a60a74bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
